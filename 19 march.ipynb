{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ff7240-354a-4ced-ab7a-6d57c1d456ec",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illu\n",
    "strate its application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ac05a-9fb6-4ad3-a10b-c16b96e68bfb",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used in machine learning to scale numerical features to a specific range, typically between 0 and 1. It transforms the data in such a way that the smallest value in the dataset is mapped to 0, and the largest value is mapped to 1, with all other values scaled proportionally in between."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02014faf-c6aa-4034-bfe8-fc8baf647eb5",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?  Provide an example to illustrate its application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39e5aa-6189-49e1-a83a-6e0cb248081b",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization, is a feature scaling method that scales each feature such that the magnitude (length) of the feature vector is equal to 1. This is achieved by dividing each data point by the Euclidean norm (magnitude) of the feature vector.\n",
    "Differences from Min-Max Scaling:\n",
    "\n",
    "Range of Values:\n",
    "\n",
    "Unit Vector: Scales the feature vector to have a magnitude of 1, but does not constrain the values to a specific range.\n",
    "Min-Max Scaling: Scales the feature values to a specific range, typically between 0 and 1.\n",
    "Effect on Magnitude:\n",
    "\n",
    "Unit Vector: Ensures that the magnitude of the feature vector is 1.\n",
    "Min-Max Scaling: Adjusts the range of feature values while maintaining the relative distances between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67d599-3e65-4cab-817a-267c1d7457ee",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an  example to illustrate its application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113e91f-1262-45e0-a159-144ec9aaf42a",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a technique used in machine learning and statistics for dimensionality reduction. It aims to reduce the complexity of high-dimensional data while preserving its essential characteristics. PCA works by transforming the original features into a new set of orthogonal (uncorrelated) features called principal components. These components are ordered by the amount of variance they explain, with the first component capturing the most variance, the second capturing the second most, and so on.\n",
    "\n",
    "PCA achieves dimensionality reduction by discarding the components with the least variance, as they contribute less information to the data. The retained principal components represent a lower-dimensional subspace that still captures a significant portion of the original data's variability.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say you have a dataset with three features: height, weight, and age of individuals. You want to reduce the dimensionality while retaining as much information as possible. By applying PCA, you can find the directions in which the data varies the most (the principal components).\n",
    "\n",
    "Suppose the first principal component (PC1) corresponds to a combination of height and weight, while the second principal component (PC2) mainly represents age. If the variability in age is relatively small compared to the variability in height and weight, then you might choose to retain only PC1 and PC2, effectively reducing the dataset from three dimensions to two.\n",
    "\n",
    "This reduction can be highly beneficial for tasks like visualization, where it's easier to understand and analyze data in lower-dimensional spaces, and for improving the performance of machine learning algorithms by reducing the risk of overfitting and speeding up computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc30a9-d44e-4a17-b393-a43c2e6e8e08",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature  Extraction? Provide an example to illustrate this concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29582cec-180d-4468-b2bf-600897699c3e",
   "metadata": {},
   "source": [
    "Relationship between PCA and Feature Extraction:\n",
    "\n",
    "In feature extraction, the goal is to derive new features from the original ones while retaining as much information as possible. PCA achieves this by finding linear combinations of the original features that capture the maximum variance in the data. These linear combinations, known as principal components, can be used as new features in a reduced-dimensional space.\n",
    "\n",
    "Using PCA for Feature Extraction:\n",
    "\n",
    "Compute the Covariance Matrix:\n",
    "\n",
    "Given a dataset with \n",
    "�\n",
    "n features, PCA starts by computing the covariance matrix of the original features. This matrix represents the relationships between different features.\n",
    "Eigenvalue Decomposition:\n",
    "\n",
    "Next, PCA performs eigenvalue decomposition on the covariance matrix. This yields eigenvectors and eigenvalues. The eigenvectors represent the directions (principal components) in which the data varies the most, and the eigenvalues indicate the amount of variance captured along those directions.\n",
    "Select Principal Components:\n",
    "\n",
    "The principal components corresponding to the highest eigenvalues capture the most variance in the data. By selecting a subset of these components, you can reduce the dimensionality of the data while retaining a significant amount of information.\n",
    "Example:\n",
    "\n",
    "Suppose you have a dataset with three features: \n",
    "�\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    " , \n",
    "�\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    " , and \n",
    "�\n",
    "3\n",
    "X \n",
    "3\n",
    "​\n",
    " , and you want to reduce it to two dimensions using PCA.\n",
    "\n",
    "Compute Covariance Matrix:\n",
    "\n",
    "Calculate the covariance matrix of the original features.\n",
    "Eigenvalue Decomposition:\n",
    "\n",
    "Perform eigenvalue decomposition to obtain eigenvectors and eigenvalues.\n",
    "Select Principal Components:\n",
    "\n",
    "Choose the top two eigenvectors (principal components) with the highest eigenvalues. These represent the directions of maximum variance in the data.\n",
    "Transform Data:\n",
    "\n",
    "Use the selected eigenvectors to project the original data onto the new reduced-dimensional space.\n",
    "The resulting transformed data will have two features (the selected principal components) instead of the original three, while still preserving a significant amount of the original information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f30f8-0e0b-4c7c-adb7-1e96953f8fca",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a23f8-b283-464c-b501-6c5ea6dfe68e",
   "metadata": {},
   "source": [
    "First, I would identify the minimum and maximum values for each of these features within the dataset. For example, for the \"price\" feature, I would find the lowest and highest prices among all the food items. Similarly, I would determine the minimum and maximum values for \"rating\" and \"delivery time\".\n",
    "\n",
    "Next, I would apply the Min-Max scaling formula to each feature. This involves transforming the values in such a way that the lowest value becomes 0 and the highest value becomes 1, with all other values scaled proportionally in between. This ensures that all the features are on a similar scale, which is crucial for many machine learning algorithms to perform effectively.\n",
    "\n",
    "By using Min-Max scaling, I am making sure that each feature contributes proportionally to the recommendation process, regardless of its original range. This way, the recommendation system can effectively consider factors like price, rating, and delivery time without any single feature dominating the others due to differences in their magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56410fa-3fee-4839-a326-0f32a64f1647",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10b590-f1ca-47db-8e1b-5a2cb744d382",
   "metadata": {},
   "source": [
    "Data Preparation:\n",
    "\n",
    "Collect and preprocess the dataset, including features like company financial data and market trends.\n",
    "Standardization or Normalization:\n",
    "\n",
    "Standardize or normalize the features to ensure they are on a similar scale, which is important for PCA.\n",
    "Calculate Covariance Matrix:\n",
    "\n",
    "Compute the covariance matrix of the standardized features. This matrix describes the relationships between different features.\n",
    "Eigenvalue and Eigenvector Computation:\n",
    "\n",
    "Find the eigenvalues and corresponding eigenvectors of the covariance matrix. These represent the directions of maximum variance in the data.\n",
    "Select Principal Components:\n",
    "\n",
    "Sort the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues capture the most variance and are referred to as principal components. You choose a number of principal components based on how much variance you want to retain.\n",
    "Projection:\n",
    "\n",
    "Project the original data onto the selected principal components. This effectively transforms the data into a lower-dimensional space.\n",
    "Model Building:\n",
    "\n",
    "Use the lower-dimensional dataset (transformed by PCA) as input for your stock price prediction model. The reduced set of features can potentially improve the model's performance and reduce computational complexity.\n",
    "Inverse Transformation (Optional):\n",
    "\n",
    "If needed, you can also perform an inverse transformation to map the predictions back to the original feature space for interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3b797-3d25-4fea-9d5f-2e7f8b1dd6e5",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bd4df3-47ca-4ff3-a845-17b2a55fddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def scaling(data):\n",
    "    min_val=min(data)\n",
    "    max_val=max(data)\n",
    "    scaled_data=[((x - min_val) / (max_val - min_val)) * 2 - 1 for x in data]\n",
    "    return scaled_data\n",
    "data=[1,5,10,15,20]\n",
    "scaled_data=scaling(data)\n",
    "print(scaled_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
